---
title:          "From Parameters to Prompts: Understanding and Mitigating the Factuality Gap between Fine-Tuned LLMs"
date:           2025-05-28 00:00:00 +0800
selected:       true
pub:            "Preprint"
# pub_pre:        "Submitted to "
pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
pub_date:       "2025"

abstract: >-
 We revisit how supervised fine-tuning affects factual knowledge in LLMs, revealing a factuality gap between known and unknown knowledge. This gap can be mitigated at inference via in-context learning (ICL) or out-of-distribution prompts. Our theoretical and empirical results show that test-time prompts can overshadow fine-tuning data, suggesting ICL can compensate for poor fine-tuning and should be considered in evaluating fine-tuning strategies.
cover:          /assets/images/covers/2025pub1.jpg
authors:
  - Xuan Gong
  - Hanbo Huang
  - Shiyu Liang#
links:
  Paper: https://github.com/luost26/bubble-visual-hash
---
